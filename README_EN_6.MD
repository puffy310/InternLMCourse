I didn’t even know OpenCompass was made by Shanghai AI Lab.
Need for Open Compass;
Having a centralized, fair evaluation greatly improves a customer’s ability to fairly compare AI Models. In certain types of evaluations it is better to use a human, although sometimes it is not available so you would need to compromise. 
OpenCompass aims to be a comprehensive evaluation harness to benchmark models in a number of tasks in different domains. OpenCompass is compatible with APIs, so it doesn’t have to be a specific model. 
OpenMM also has some new benchmarks that are more domain specific now.
Data contamination:
In a lot of situations test-set data might be put into the training data. 
Usage guide:
Lots of different benchmarks and tools. Data contamination and performance evaluations are the most common. Each eval is in a special format and it is relatively easy to modify. In order to use, you must be in the correct path. 
In the event of using an InternLM model, it is a drop in configuration, although for other models it may be different.
Will be looking through the tutorial too!
